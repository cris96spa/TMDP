{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "from TMDP import TMDP\n",
    "from algorithms import *\n",
    "from model_functions import *\n",
    "from policy_utils import *\n",
    "from experiment_result_utils import *\n",
    "from constants import *\n",
    "\n",
    "from FrozenLake import *\n",
    "from CurriculumQ import CurriculumQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_slippery = False\n",
    "reward_shape = True\n",
    "dense_reward = True\n",
    "num_bins = 15\n",
    "nrows = 30\n",
    "\n",
    "num_runs = 10\n",
    "episodes = 4900000\n",
    "checkpoint_step=500\n",
    "test_episodes = 1000\n",
    "\n",
    "shape_range=(-1,0)\n",
    "goal_reward = 1.\n",
    "debug = False\n",
    "param_decay=True\n",
    "\n",
    "lam = 1\n",
    "experiment_results = []\n",
    "tests_returns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Frozen Lake Environment\n",
    "tau = 0.6\n",
    "nS = nrows**2\n",
    "nA = 4\n",
    "gamma = 0.999\n",
    "xi = np.ones(nS) * 1/nS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = 0.15869281717397965\n",
    "\n",
    "batch_size = 20\n",
    "exp_rate = 0.4\n",
    "eps_model = compute_eps_model(gamma, tau, episodes/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = f\"CurrQ_{num_bins}\"\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "experiment_name = f\"FrozenLake_{nrows}x{nrows}_{num_bins}\"\n",
    "experiment_id = get_or_create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "save_path = f\"results/{experiment_name}/run_{run_name}\"\n",
    "label = run_name.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(index, seed, run_name, change_map=False):\n",
    "    sub_run_name = f\"{run_name}_{index}\"\n",
    "    \n",
    "    with mlflow.start_run(nested=True, run_name=sub_run_name):\n",
    "        # Environment specific configuration   \n",
    "        map_seed = seed if change_map else constants.SEEDS[0]\n",
    "        set_policy_seed(seed)\n",
    "        env = FrozenLakeEnv(is_slippery=is_slippery, seed=seed, \n",
    "                        desc=generate_random_map(nrows, seed=map_seed), \n",
    "                        reward_shape=reward_shape,\n",
    "                        num_bins=num_bins,\n",
    "                        dense_reward=dense_reward,\n",
    "                        shape_range=shape_range,\n",
    "                        goal_reward=goal_reward,\n",
    "                        )\n",
    "        \n",
    "        # Environment independent configuration\n",
    "        tmdp = TMDP(env, xi, tau=tau, gamma=gamma, seed=seed)\n",
    "        tmdp.update_tau(tau)\n",
    "        curr_Q = CurriculumQ(tmdp, checkpoint_step=checkpoint_step)\n",
    "\n",
    "        curr_Q.train(model_lr, batch_size=batch_size, \n",
    "                lam=lam, exp_rate=exp_rate,\n",
    "                episodes=episodes,\n",
    "                eps_model=eps_model,\n",
    "                param_decay=param_decay,\n",
    "                debug=debug,)\n",
    "    \n",
    "        avg_return = np.average(curr_Q.reward_records[-10:])/batch_size\n",
    "        \n",
    "        mlflow.log_metric(\"Avg Return\", avg_return)\n",
    "        \n",
    "        run_dict = {\n",
    "            \"episodes\": curr_Q.episodes,\n",
    "            \"model_lr\": model_lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"lam\": lam,\n",
    "            \"eps_model\": eps_model,\n",
    "            \"exp_rate\": exp_rate,\n",
    "        }\n",
    "        mlflow.log_params(run_dict)\n",
    "        mlflow.set_tags(tags={\n",
    "            \"run_name\": run_name,\n",
    "            \"change_map\": change_map,\n",
    "            \"seed\": seed,\n",
    "            \"tau\": tau,\n",
    "            \"gamma\": gamma,\n",
    "            \"checkpoint_step\": checkpoint_step,\n",
    "            \"test_episodes\": test_episodes,\n",
    "            \"index\": index,\n",
    "            \"dense_reward\": dense_reward,\n",
    "            \"shape_range\": shape_range,\n",
    "            \"goal_reward\": goal_reward,\n",
    "            \"reward_shape\": reward_shape,\n",
    "        })\n",
    "\n",
    "        \n",
    "        test_policies_return = test_Q_policies(tmdp, curr_Q.Qs, test_episodes)\n",
    "        \n",
    "        result_dict = {\n",
    "            \"Qs\" : curr_Q.Qs,\n",
    "            \"taus\" : curr_Q.taus,\n",
    "            \"reward_records\" : curr_Q.reward_records,\n",
    "            \"test_policies_return\" : test_policies_return,\n",
    "            \"index\" : index,\n",
    "        }\n",
    "\n",
    "        tests_returns.append(test_policies_return)\n",
    "        experiment_results.append(result_dict)\n",
    "        # Save artifact to MLFlow\n",
    "        try:\n",
    "            save_to_mlflow(result_dict)\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving to MLFlow: {e}\")\n",
    "            print(\"Saving locally instead.\")\n",
    "            time.sleep(5)\n",
    "            path = save_path+f\"/{sub_run_name}\"\n",
    "            save(path, result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(change_map=False, num_runs=10):\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            seed = constants.SEEDS[i]\n",
    "            run_experiment(i, seed, run_name, change_map)\n",
    "        \n",
    "        experiment_dict = {\n",
    "            \"tests_returns\": tests_returns,\n",
    "            \"num_runs\": num_runs,\n",
    "            \"change_map\": change_map,\n",
    "            \"num_bins\": num_bins,\n",
    "            \"label\": label,\n",
    "        }\n",
    "        try:\n",
    "            save_to_mlflow(experiment_dict)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Something went wrong saving the experiment results to MLFlow.\")\n",
    "            print(\"Saving locally instead.\")\n",
    "            time.sleep(5)\n",
    "            save(save_path, experiment_dict)\n",
    "\n",
    "        rewards_fig = plot_avg_test_return(tests_returns, f\"{run_name[:-3]} Avg Rewards on {num_runs} runs\")\n",
    "        try:\n",
    "            mlflow.log_figure(figure=rewards_fig, artifact_file=\"reward_image.png\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Something went wrong saving the figure to MLFlow.\")\n",
    "            print(\"Saving locally instead.\")\n",
    "            time.sleep(5)\n",
    "            rewards_fig.savefig(save_path+\"/reward_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current seed for result reproducibility: 2999\n",
      "Episode: 5000 reward: -4269.866666666667 length: 99\n",
      "Episode: 10000 reward: -2414.4 length: 69\n",
      "Episode: 15000 reward: -3325.5999999999985 length: 133\n",
      "Episode: 20000 reward: -865.0666666666668 length: 45\n",
      "Episode: 25000 reward: -1129.7333333333333 length: 4\n",
      "Episode: 30000 reward: -1912.0666666666666 length: 48\n",
      "Episode: 35000 reward: -2220.6666666666665 length: 1\n",
      "Episode: 40000 reward: -1534.8666666666675 length: 50\n",
      "Episode: 45000 reward: -680.2 length: 26\n",
      "Episode: 50000 reward: -287.4 length: 109\n",
      "Episode: 55000 reward: -2221.866666666667 length: 1\n",
      "Episode: 60000 reward: -1337.9333333333332 length: 48\n",
      "Episode: 65000 reward: -988.9999999999999 length: 103\n",
      "Episode: 70000 reward: -677.7333333333332 length: 188\n",
      "Episode: 75000 reward: -220.80000000000015 length: 56\n",
      "Episode: 80000 reward: -362.66666666666663 length: 61\n",
      "Episode: 85000 reward: -2345.2666666666664 length: 53\n",
      "Episode: 90000 reward: -2030.666666666667 length: 48\n",
      "Episode: 95000 reward: -2096.399999999999 length: 120\n",
      "Episode: 100000 reward: -941.6666666666666 length: 73\n",
      "Episode: 105000 reward: -2088.9333333333334 length: 18\n",
      "Episode: 110000 reward: -1802.7333333333343 length: 78\n",
      "Episode: 115000 reward: -2744.733333333333 length: 149\n",
      "Episode: 120000 reward: -871.5333333333332 length: 147\n",
      "Episode: 125000 reward: -3379.200000000001 length: 12\n",
      "Episode: 130000 reward: -2116.666666666666 length: 27\n",
      "Episode: 135000 reward: -1262.5999999999995 length: 12\n",
      "Episode: 140000 reward: -1259.9333333333336 length: 17\n",
      "Episode: 145000 reward: -731.1333333333332 length: 16\n",
      "Episode: 150000 reward: -1564.333333333334 length: 50\n",
      "Episode: 155000 reward: -2744.1333333333337 length: 66\n",
      "Episode: 160000 reward: -2311.6666666666674 length: 102\n",
      "Episode: 165000 reward: -689.066666666667 length: 84\n",
      "Episode: 170000 reward: -1534.3999999999999 length: 70\n",
      "Episode: 175000 reward: -2038.1333333333332 length: 0\n",
      "Episode: 180000 reward: -1562.0 length: 19\n",
      "Episode: 185000 reward: -661.8666666666667 length: 158\n",
      "Episode: 190000 reward: -1194.799999999999 length: 43\n",
      "Episode: 195000 reward: -2245.4666666666662 length: 141\n",
      "Episode: 200000 reward: -45.066666666666656 length: 106\n",
      "Episode: 205000 reward: -809.5333333333332 length: 45\n",
      "Episode: 210000 reward: -1386.0666666666664 length: 124\n",
      "Episode: 215000 reward: -1831.6666666666658 length: 94\n",
      "Episode: 220000 reward: -1379.5333333333333 length: 58\n",
      "Episode: 225000 reward: -1994.6666666666658 length: 73\n",
      "Episode: 230000 reward: -1640.3333333333328 length: 90\n",
      "Episode: 235000 reward: -37.6 length: 107\n",
      "Episode: 240000 reward: -165.33333333333331 length: 119\n",
      "Episode: 245000 reward: -1260.6666666666677 length: 29\n",
      "Episode: 250000 reward: -47.533333333333324 length: 4\n",
      "Episode: 255000 reward: -1114.4 length: 144\n",
      "Episode: 260000 reward: -1636.1999999999998 length: 63\n",
      "Episode: 265000 reward: -602.2666666666668 length: 0\n",
      "Episode: 270000 reward: -878.4666666666666 length: 138\n",
      "Episode: 275000 reward: -31.266666666666662 length: 44\n",
      "Episode: 280000 reward: -1196.9333333333334 length: 150\n",
      "Episode: 285000 reward: -1318.3333333333337 length: 37\n",
      "Episode: 290000 reward: -1383.8 length: 116\n",
      "Episode: 295000 reward: -1190.5333333333333 length: 138\n",
      "Episode: 300000 reward: -1515.8666666666668 length: 68\n",
      "Episode: 305000 reward: -2279.666666666667 length: 85\n",
      "Episode: 310000 reward: -1265.7999999999995 length: 10\n",
      "Episode: 315000 reward: -1179.5333333333335 length: 105\n",
      "Episode: 320000 reward: -992.8666666666666 length: 133\n",
      "Episode: 325000 reward: -1451.733333333334 length: 51\n",
      "Episode: 330000 reward: -3182.6000000000004 length: 49\n",
      "Episode: 335000 reward: -1849.333333333333 length: 19\n",
      "Episode: 340000 reward: -2425.133333333334 length: 117\n",
      "Episode: 345000 reward: -1659.2000000000003 length: 14\n",
      "Episode: 350000 reward: -1075.533333333334 length: 8\n",
      "Episode: 355000 reward: -2368.5999999999995 length: 5\n",
      "Episode: 360000 reward: -2498.266666666667 length: 82\n",
      "Episode: 365000 reward: -2473.4 length: 123\n",
      "Episode: 370000 reward: -2555.5333333333338 length: 125\n",
      "Episode: 375000 reward: -2998.0 length: 104\n",
      "Episode: 380000 reward: -714.6666666666667 length: 15\n",
      "Episode: 385000 reward: -1566.3333333333337 length: 56\n",
      "Episode: 390000 reward: -2036.8 length: 0\n",
      "Episode: 395000 reward: -1393.133333333332 length: 36\n",
      "Episode: 400000 reward: -2613.666666666667 length: 130\n",
      "Episode: 405000 reward: -688.0666666666665 length: 113\n",
      "Episode: 410000 reward: -1518.466666666667 length: 141\n",
      "Episode: 415000 reward: -543.6666666666665 length: 63\n",
      "Episode: 420000 reward: -95.86666666666667 length: 74\n",
      "Episode: 425000 reward: -1591.7999999999995 length: 61\n",
      "Episode: 430000 reward: -231.00000000000003 length: 17\n",
      "Episode: 435000 reward: -1014.7333333333329 length: 82\n",
      "Episode: 440000 reward: -1902.8000000000006 length: 160\n",
      "Episode: 445000 reward: -1706.1333333333332 length: 105\n",
      "Episode: 450000 reward: -2624.933333333332 length: 96\n",
      "Episode: 455000 reward: -2291.0000000000005 length: 14\n",
      "Episode: 460000 reward: -1829.1333333333332 length: 138\n",
      "Episode: 465000 reward: -104.06666666666662 length: 105\n",
      "Episode: 470000 reward: -1321.533333333333 length: 127\n",
      "Episode: 475000 reward: -2430.7333333333345 length: 146\n",
      "Episode: 480000 reward: -2169.933333333333 length: 43\n",
      "Episode: 485000 reward: -1333.1333333333334 length: 94\n",
      "Episode: 490000 reward: -2476.9333333333334 length: 79\n",
      "Episode: 495000 reward: -1852.9333333333332 length: 108\n",
      "Episode: 500000 reward: -428.3333333333333 length: 147\n",
      "Episode: 505000 reward: -2422.9333333333334 length: 75\n",
      "Episode: 510000 reward: -2418.4666666666662 length: 1\n",
      "Episode: 515000 reward: -2363.9333333333348 length: 109\n",
      "Episode: 520000 reward: -1660.6000000000004 length: 142\n",
      "Episode: 525000 reward: -686.2666666666667 length: 125\n",
      "Episode: 530000 reward: -1288.3999999999999 length: 111\n",
      "Episode: 535000 reward: -1195.0666666666664 length: 46\n",
      "Episode: 540000 reward: -2950.4666666666653 length: 20\n",
      "Episode: 545000 reward: -2683.6 length: 129\n",
      "Episode: 550000 reward: -1078.8 length: 117\n",
      "Episode: 555000 reward: -1654.8666666666663 length: 65\n",
      "Episode: 560000 reward: -620.8000000000005 length: 63\n",
      "Episode: 565000 reward: -1930.666666666667 length: 34\n",
      "Episode: 570000 reward: -20.866666666666667 length: 128\n",
      "Episode: 575000 reward: -2365.6000000000004 length: 88\n",
      "Episode: 580000 reward: -246.26666666666665 length: 61\n",
      "Episode: 585000 reward: -1783.8666666666666 length: 107\n",
      "Episode: 590000 reward: -741.7333333333333 length: 50\n",
      "Episode: 595000 reward: -688.8 length: 13\n",
      "Episode: 600000 reward: -1790.3333333333335 length: 151\n",
      "Episode: 605000 reward: -647.9999999999999 length: 106\n",
      "Episode: 610000 reward: -2351.666666666667 length: 106\n",
      "Episode: 615000 reward: -2612.9999999999995 length: 9\n",
      "Episode: 620000 reward: -1708.8666666666668 length: 139\n",
      "Episode: 625000 reward: -1138.133333333333 length: 33\n",
      "Episode: 630000 reward: -1576.1333333333325 length: 207\n",
      "Episode: 635000 reward: -1209.6666666666665 length: 98\n",
      "Episode: 640000 reward: -2282.5333333333333 length: 114\n",
      "Episode: 645000 reward: -1346.8666666666666 length: 25\n",
      "Episode: 650000 reward: -3136.133333333333 length: 88\n",
      "Episode: 655000 reward: -1595.0666666666662 length: 24\n",
      "Episode: 660000 reward: -332.9333333333331 length: 173\n",
      "Episode: 665000 reward: -688.6666666666665 length: 57\n",
      "Episode: 670000 reward: -1588.5333333333326 length: 11\n",
      "Episode: 675000 reward: -1326.9333333333336 length: 98\n",
      "Episode: 680000 reward: -2685.0666666666657 length: 158\n",
      "Episode: 685000 reward: -482.66666666666663 length: 28\n",
      "Episode: 690000 reward: -2090.8000000000006 length: 58\n",
      "Episode: 695000 reward: -1275.2 length: 82\n",
      "Episode: 700000 reward: -2381.000000000001 length: 48\n",
      "Episode: 705000 reward: -2034.7333333333331 length: 139\n",
      "Episode: 710000 reward: -751.7999999999997 length: 98\n",
      "Episode: 715000 reward: -1354.4666666666672 length: 78\n",
      "Episode: 720000 reward: -1397.3999999999996 length: 8\n",
      "Episode: 725000 reward: -2482.866666666667 length: 94\n",
      "Episode: 730000 reward: -4690.200000000001 length: 62\n",
      "Episode: 735000 reward: -1784.8000000000002 length: 102\n",
      "Episode: 740000 reward: -2499.2000000000003 length: 78\n",
      "Episode: 745000 reward: -880.2666666666663 length: 38\n",
      "Episode: 750000 reward: -2953.0666666666657 length: 68\n",
      "Episode: 755000 reward: -1967.7333333333331 length: 34\n",
      "Episode: 760000 reward: -2545.2666666666664 length: 94\n",
      "Episode: 765000 reward: -1251.6666666666667 length: 143\n",
      "Episode: 770000 reward: -950.3333333333334 length: 137\n",
      "Episode: 775000 reward: -2424.6000000000013 length: 34\n",
      "Episode: 780000 reward: -1593.5333333333333 length: 105\n",
      "Episode: 785000 reward: -1856.4000000000005 length: 135\n",
      "Episode: 790000 reward: -1535.8 length: 108\n",
      "Episode: 795000 reward: -1845.6666666666656 length: 92\n",
      "Episode: 800000 reward: -752.4000000000001 length: 133\n",
      "Episode: 805000 reward: -1735.5333333333326 length: 111\n",
      "Episode: 810000 reward: -2415.6666666666674 length: 32\n",
      "Episode: 815000 reward: -3923.666666666667 length: 9\n",
      "Episode: 820000 reward: -2880.7999999999997 length: 2\n",
      "Episode: 825000 reward: -2119.0666666666666 length: 183\n",
      "Episode: 830000 reward: -2207.800000000001 length: 75\n",
      "Episode: 835000 reward: -4046.7333333333336 length: 46\n",
      "Episode: 840000 reward: -1472.4666666666667 length: 156\n",
      "Episode: 845000 reward: -1022.1333333333332 length: 203\n",
      "Episode: 850000 reward: -1664.466666666667 length: 99\n",
      "Episode: 855000 reward: -1917.3333333333335 length: 8\n",
      "Episode: 860000 reward: -2431.4 length: 14\n",
      "Episode: 865000 reward: -1915.866666666666 length: 74\n",
      "Episode: 870000 reward: -1652.533333333333 length: 66\n",
      "Episode: 875000 reward: -1645.5333333333335 length: 68\n"
     ]
    }
   ],
   "source": [
    "run_experiments(change_map=False, num_runs=num_runs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
