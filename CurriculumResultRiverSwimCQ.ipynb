{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import math\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import optuna\n",
    "import os\n",
    "\n",
    "from TMDP import TMDP\n",
    "from algorithms import *\n",
    "from model_functions import *\n",
    "from policy_utils import *\n",
    "from experiment_result_utils import *\n",
    "from constants import *\n",
    "\n",
    "from RiverSwim import *\n",
    "from CurriculumQ import CurriculumQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#River Swim Environment\n",
    "nS = 100\n",
    "uniform_restart = False\n",
    "num_runs = 2\n",
    "\n",
    "\n",
    "small = 5e-3\n",
    "large = 1.\n",
    "nA = 2\n",
    "gamma = 0.999\n",
    "\n",
    "if uniform_restart:\n",
    "    mu = np.ones(nS) / nS\n",
    "else:\n",
    "    mu = np.zeros(nS)\n",
    "    mu[1] = 1 \n",
    "\n",
    "xi = np.ones(nS) * 1/nS\n",
    "\n",
    "episodes = 15000000\n",
    "checkpoint_step=500\n",
    "test_episodes = 10000\n",
    "\n",
    "param_decay=True\n",
    "debug = False\n",
    "\n",
    "lam = 1\n",
    "experiment_results = []\n",
    "tests_returns = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = 0.15869281717397965\n",
    "tau = 0.6\n",
    "batch_size = 20\n",
    "exp_rate = 0.4\n",
    "eps_model = compute_eps_model(gamma, tau, episodes/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "run_name = f\"CurrQ_{uniform_restart}\"\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_URI)\n",
    "experiment_name = f\"RiverSwim_{nS}_{uniform_restart}\"\n",
    "experiment_id = get_or_create_experiment(experiment_name)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "save_path = f\"results/{experiment_name}/run_{run_name}\"\n",
    "label = run_name.split(\"_\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(index, seed, run_name):\n",
    "    sub_run_name = f\"{run_name}_{index}\"\n",
    "    \n",
    "    with mlflow.start_run(nested=True, run_name=sub_run_name):\n",
    "        # Environment specific configuration   \n",
    "        \n",
    "        set_policy_seed(seed)\n",
    "        env = RiverSwim(nS, mu, small=small, large=large, seed=seed)\n",
    "        \n",
    "        # Environment independent configuration\n",
    "        tmdp = TMDP(env, xi, tau=tau, gamma=gamma, seed=seed)\n",
    "        tmdp.update_tau(tau)\n",
    "        curr_Q = CurriculumQ(tmdp, checkpoint_step=checkpoint_step)\n",
    "\n",
    "        curr_Q.train(model_lr, batch_size=batch_size, \n",
    "                lam=lam, exp_rate=exp_rate,\n",
    "                episodes=episodes,\n",
    "                eps_model=eps_model,\n",
    "                param_decay=param_decay,\n",
    "                debug=debug,)\n",
    "    \n",
    "        avg_return = np.average(curr_Q.reward_records[-10:])/batch_size\n",
    "        \n",
    "        mlflow.log_metric(\"Avg Return\", avg_return)\n",
    "        \n",
    "        test_policies_return = test_Q_policies(tmdp, curr_Q.Qs, test_episodes)\n",
    "        \n",
    "        result_dict = {\n",
    "            \"Qs\" : curr_Q.Qs,\n",
    "            \"taus\" : curr_Q.taus,\n",
    "            \"reward_records\" : curr_Q.reward_records,\n",
    "            \"test_policies_return\" : test_policies_return,\n",
    "            \"index\" : index,\n",
    "        }\n",
    "\n",
    "        tests_returns.append(test_policies_return)\n",
    "        experiment_results.append(result_dict)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(num_runs=10):\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            seed = constants.SEEDS[i]\n",
    "            run_experiment(i, seed, run_name)\n",
    "        \n",
    "        experiment_dict = {\n",
    "            \"tests_returns\": tests_returns,\n",
    "            \"num_runs\": num_runs,\n",
    "            \"label\": label,\n",
    "            \"checkpoint_step\": checkpoint_step,\n",
    "        }\n",
    "        mlflow.set_tags(tags={\n",
    "            \"seed\": seed,\n",
    "            \"tau\": tau,\n",
    "            \"gamma\": gamma,\n",
    "            \"checkpoint_step\": checkpoint_step,\n",
    "            \"test_episodes\": test_episodes,\n",
    "            \"uniform_restart\": uniform_restart,\n",
    "            \"episodes\": episodes,\n",
    "            \"model_lr\": model_lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"lam\": lam,\n",
    "            \"eps_model\": eps_model,\n",
    "            \"exp_rate\": exp_rate,\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            save_to_mlflow(experiment_dict)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Something went wrong saving the experiment results to MLFlow.\")\n",
    "            print(\"Saving locally instead.\")\n",
    "            time.sleep(5)\n",
    "            save(save_path, experiment_dict)\n",
    "\n",
    "        rewards_fig = plot_avg_test_return(tests_returns, f\"{run_name[:-3]} Avg Return on {num_runs} runs\")\n",
    "        try:\n",
    "            mlflow.log_figure(figure=rewards_fig, artifact_file=\"reward_image.png\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Something went wrong saving the figure to MLFlow.\")\n",
    "            print(\"Saving locally instead.\")\n",
    "            time.sleep(5)\n",
    "            rewards_fig.savefig(save_path+\"/reward_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiments(num_runs=10):\n",
    "    with mlflow.start_run(run_name=run_name):\n",
    "        \n",
    "        for i in range(num_runs):\n",
    "            seed = constants.SEEDS[i]\n",
    "            run_experiment(i, seed, run_name)\n",
    "        \n",
    "        experiment_dict = {\n",
    "            \"tests_returns\": tests_returns,\n",
    "            \"num_runs\": num_runs,\n",
    "            \"label\": label,\n",
    "            \"checkpoint_step\": checkpoint_step,\n",
    "        }\n",
    "        mlflow.set_tags(tags={\n",
    "            \"seed\": seed,\n",
    "            \"tau\": tau,\n",
    "            \"gamma\": gamma,\n",
    "            \"checkpoint_step\": checkpoint_step,\n",
    "            \"test_episodes\": test_episodes,\n",
    "            \"uniform_restart\": uniform_restart,\n",
    "            \"episodes\": episodes,\n",
    "            \"model_lr\": model_lr,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"lam\": lam,\n",
    "            \"eps_model\": eps_model,\n",
    "            \"exp_rate\": exp_rate,\n",
    "        })\n",
    "        \n",
    "        try:\n",
    "            save_to_mlflow(experiment_dict)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Something went wrong saving the experiment results to MLFlow.\")\n",
    "            print(\"Saving locally instead.\")\n",
    "            time.sleep(5)\n",
    "            save(save_path, experiment_dict)\n",
    "\n",
    "        rewards_fig = plot_avg_test_return(tests_returns, f\"{run_name[:-3]} Avg Return on {num_runs} runs\")\n",
    "        try:\n",
    "            mlflow.log_figure(figure=rewards_fig, artifact_file=\"reward_image.png\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"Something went wrong saving the figure to MLFlow.\")\n",
    "            print(\"Saving locally instead.\")\n",
    "            time.sleep(5)\n",
    "            rewards_fig.savefig(save_path+\"/reward_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiments(num_runs=num_runs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
