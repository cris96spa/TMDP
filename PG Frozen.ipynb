{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current seed for result reproducibility: 53166883074729279014800260307620810660\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from TMDP import TMDP\n",
    "from River_swim import River\n",
    "\n",
    "from algorithms import *\n",
    "from model_functions import *\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from FrozenLake import *\n",
    "\n",
    "#np.set_printoptions(precision=4)\n",
    "import math\n",
    "from utils import *\n",
    "\n",
    "nrows = 20\n",
    "nS = nrows**2\n",
    "nA = 4\n",
    "seed = get_current_seed()\n",
    "seed = 44697628841978080856580175700798794719\n",
    "gamma = .9\n",
    "tau = 1.\n",
    "env = FrozenLakeEnv(is_slippery=False, seed=seed, desc=generate_random_map(nrows))#, render_mode=\"human\")\n",
    "xi_frozen = np.ones(env.nS) * 1/env.nS\n",
    "env.reset()\n",
    "xi = np.ones(nS) * 1/nS\n",
    "tmdp = TMDP(env, xi_frozen, tau=tau, gamma=gamma, seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class PolicyPi(nn.Module):\n",
    "    def __init__(self, nS, nA, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        # Embedding layer to handle integer states\n",
    "        self.embedding = nn.Embedding(nS, nS)\n",
    "        self.hidden = nn.Linear(nS, hidden_dim)\n",
    "        self.classify = nn.Linear(hidden_dim, nA)\n",
    "\n",
    "    def forward(self, s):\n",
    "        # Embed the integer state\n",
    "        s = self.embedding(s)\n",
    "        s = F.relu(self.hidden(s))\n",
    "        logits = self.classify(s)\n",
    "        return logits\n",
    "    \n",
    "    def act(self, s):\n",
    "        with torch.no_grad():\n",
    "            # Preparing the input state tensor\n",
    "            s = torch.tensor([s], dtype=torch.long).to(device)  # Wrap integer state in a list, convert to tensor\n",
    "            logits = policy_pi(s)\n",
    "            logits = logits.squeeze(dim=0)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            action = torch.multinomial(probs, num_samples=1)\n",
    "            return action.item()\n",
    "    \n",
    "    def get_probabilities(self):\n",
    "        probs = np.zeros((nS, nA))\n",
    "        with torch.no_grad():\n",
    "            for s in range(nS):\n",
    "                s = torch.tensor([s], dtype=torch.long).to(device)\n",
    "                logits = policy_pi(s)\n",
    "                logits = logits.squeeze(dim=0)\n",
    "                probs[s] = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "        return probs\n",
    "    \n",
    "    def get_logits(self):\n",
    "        full_logits = np.zeros((nS, nA))\n",
    "        with torch.no_grad():\n",
    "            for s in range(nS):\n",
    "                s = torch.tensor([s], dtype=torch.long).to(device)\n",
    "                logits = policy_pi(s)\n",
    "                logits = logits.squeeze(dim=0)\n",
    "                full_logits[s] = logits.cpu().numpy()\n",
    "        return full_logits\n",
    "    \n",
    "policy_pi = PolicyPi(nS, nA).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(policy_pi.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curriculum_PG(tmdp:TMDP, policy_pi:PolicyPi, opt, episodes=5000, alpha=.25, batch_nS=1, biased=True):\n",
    "    \n",
    "    nS, nA = tmdp.env.nS, tmdp.env.nA\n",
    "    done = False \n",
    "    terminated = False\n",
    "\n",
    "    batch_size = 0\n",
    "    # State action next-state value function\n",
    "    U = np.zeros((nS, nA, nS))\n",
    "    V = np.zeros(nS)\n",
    "    Q = np.zeros((nS, nA))\n",
    "\n",
    "    # Hyperparameters decay\n",
    "    dec_alpha = alpha\n",
    "    \n",
    "    # Curriculum parameters\n",
    "    alpha_star = dec_alpha\n",
    "    tau_star = 1\n",
    "    convergence_t = 0\n",
    "\n",
    "    # Mid-term results\n",
    "    rewards_records = []\n",
    "    states = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    flags_list = []\n",
    "    t = 0\n",
    "    for episode in range(episodes): # Each episode is a single time step\n",
    "        \n",
    "        while True:\n",
    "            s = tmdp.env.s\n",
    "            states.append(s)\n",
    "            a = policy_pi.act(s) # Select action from policy\n",
    "\n",
    "            s_prime, r, flags, p =  tmdp.step(a)\n",
    "\n",
    "            flags[\"terminated\"] = terminated\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            flags_list.append(flags)\n",
    "\n",
    "            # Reset the environment if a terminal state is reached or if a teleportation happened\n",
    "            if flags[\"done\"]:# or flags[\"teleport\"]:\n",
    "                tmdp.reset()\n",
    "                batch_size += 1\n",
    "\n",
    "            if episode < episodes-1: # move to next time step\n",
    "                break   \n",
    "            else: # if reached the max num of time steps, wait for the end of the trajectory for consistency\n",
    "                print(\"Ending the loop\")\n",
    "                terminated = True\n",
    "                flags[\"terminated\"] = terminated\n",
    "                break # temporary ending condition To be Removed\n",
    "                if flags[\"done\"]:\n",
    "                    done = True\n",
    "                    break\n",
    "\n",
    "        # Processing the batch\n",
    "        if( (batch_size != 0 and batch_size % batch_nS == 0) or done or terminated):\n",
    "            # Extract previous policy for future comparison\n",
    "            \n",
    "            cum_reward = np.zeros_like(rewards)\n",
    "            reward_len = len(rewards)\n",
    "            for i in reversed(range(reward_len)):\n",
    "                cum_reward[i] = rewards[i] + tmdp.gamma*cum_reward[i+1] if i < reward_len-1 else rewards[i]\n",
    "                \n",
    "            logits_old = policy_pi.get_logits()\n",
    "            pi_old = policy_pi.get_probabilities()\n",
    "\n",
    "            tensor_states = torch.tensor(states, dtype=torch.long).to(device)\n",
    "            tensor_actions = torch.tensor(actions, dtype=torch.long).to(device)\n",
    "            tensor_cum_reward = torch.tensor(cum_reward, dtype=torch.float).to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = policy_pi(tensor_states)\n",
    "            # Calculate negative log probability (-log P) as loss.\n",
    "            # Cross-entropy loss is -log P in categorical distribution.\n",
    "\n",
    "            logs_prob = -F.cross_entropy(logits, tensor_actions, reduction='none')\n",
    "            loss = -logs_prob * tensor_cum_reward\n",
    "            loss.sum().backward()\n",
    "            opt.step()\n",
    "\n",
    "            # Learning the value function\n",
    "            for i in range(len(states)):\n",
    "                t += 1\n",
    "                dec_alpha= max(1e-5, alpha*(1 - t/episodes))\n",
    "                s = states[i]\n",
    "                a = actions[i]\n",
    "                s_prime = states[i+1] if i < len(states)-1 else states[i]\n",
    "                a_prime = actions[i+1] if i < len(states)-1 else pi_old[s].argmax()\n",
    "                r = rewards[i]\n",
    "                flags = flags_list[i]\n",
    "\n",
    "                td_error = dec_alpha*(r + tmdp.gamma*V[s_prime] - V[s])\n",
    "                V[s] = V[s] + dec_alpha*td_error\n",
    "                U[s,a,s_prime] = U[s,a,s_prime] + dec_alpha*(r + tmdp.gamma*V[s_prime] - U[s,a,s_prime])\n",
    "                Q[s,a] = Q[s,a] + dec_alpha*(r + tmdp.gamma*Q[s_prime, a_prime] - Q[s,a])\n",
    "\n",
    "            # Bound evaluation\n",
    "            if( tmdp.tau > 0):\n",
    "                pi = policy_pi.get_probabilities()\n",
    "                rel_pol_adv = compute_relative_policy_advantage_function(pi, pi_old, Q)\n",
    "                rel_model_adv = compute_relative_model_advantage_function(tmdp.env.P_mat, tmdp.xi, U)\n",
    "                d = compute_d_from_tau(tmdp.env.mu, tmdp.env.P_mat, tmdp.xi, pi_old, tmdp.gamma, tmdp.tau)\n",
    "                delta = compute_delta(d, pi_old)\n",
    "                pol_adv = compute_expected_policy_advantage(rel_pol_adv, d)\n",
    "                model_adv = compute_expected_model_advantage(rel_model_adv, delta)\n",
    "\n",
    "                delta_U = get_sup_difference_U(U)\n",
    "                if delta_U == 0:\n",
    "                    delta_U = (tmdp.env.reward_range[1]-tmdp.env.reward_range[0])/(1-tmdp.gamma)\n",
    "                    \n",
    "                d_inf_pol = get_d_inf_policy(pi, pi_old)\n",
    "                d_inf_model = get_d_inf_model(tmdp.env.P_mat, tmdp.xi)\n",
    "                d_exp_pol = get_d_exp_policy(pi, pi_old, d)\n",
    "                d_exp_model = get_d_exp_model(tmdp.env.P_mat, tmdp.xi, delta)\n",
    "\n",
    "                # Compute optimal values\n",
    "                optimal_pairs = get_teleport_bound_optimal_values(pol_adv, model_adv, delta_U,\n",
    "                                                                d_inf_pol, d_exp_pol, d_inf_model,\n",
    "                                                                d_exp_model, tmdp.tau, tmdp.gamma, biased=biased)\n",
    "                teleport_bounds = []\n",
    "                for alpha_prime, tau_prime in optimal_pairs:\n",
    "                    bound = compute_teleport_bound(alpha_prime, tmdp.tau, tau_prime, pol_adv, model_adv,\n",
    "                                                    tmdp.gamma, d_inf_pol, d_inf_model,\n",
    "                                                    d_exp_pol, d_exp_model, delta_U, biased=biased)\n",
    "                    teleport_bounds.append(bound)\n",
    "                \n",
    "                alpha_star, tau_star = get_teleport_bound_optima_pair(optimal_pairs, teleport_bounds)\n",
    "\n",
    "                print(optimal_pairs)\n",
    "                print(teleport_bounds)\n",
    "                print(\"Updating the policy with alpha_star: \", alpha_star, \"tau_star: \",tau_star)\n",
    "                tmdp.update_tau(tau_star)\n",
    "    \n",
    "                if tau_star == 0:\n",
    "                    print(\"Converged to the original problem, episode {}\".format(episode))\n",
    "                    convergence_t = episode\n",
    "                    # Set lambda parameter for eligibility traces for the fine tuning of the policy\n",
    "            r_sum = sum(rewards)\n",
    "            print(\"Running episode {} reward {}\".format(episode, r_sum))\n",
    "            # Reset the batch\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            flags_list = []\n",
    "            batch_size = 0\n",
    "            rewards_records.append(r_sum)\n",
    "\n",
    "    return {\"Q\": Q, \"V\": V, \"U\": U, \"policy_pi\": policy_pi, \"convergence_t\": convergence_t, \"rewards\": rewards_records}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m tmdp\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m      2\u001b[0m tmdp\u001b[38;5;241m.\u001b[39mupdate_tau(\u001b[38;5;241m.5\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m cur_res \u001b[38;5;241m=\u001b[39m \u001b[43mcurriculum_PG\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmdp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_pi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m300000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m.25\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_nS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiased\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 104\u001b[0m, in \u001b[0;36mcurriculum_PG\u001b[1;34m(tmdp, policy_pi, opt, episodes, alpha, batch_nS, biased)\u001b[0m\n\u001b[0;32m    102\u001b[0m rel_pol_adv \u001b[38;5;241m=\u001b[39m compute_relative_policy_advantage_function(pi, pi_old, Q)\n\u001b[0;32m    103\u001b[0m rel_model_adv \u001b[38;5;241m=\u001b[39m compute_relative_model_advantage_function(tmdp\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mP_mat, tmdp\u001b[38;5;241m.\u001b[39mxi, U)\n\u001b[1;32m--> 104\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_d_from_tau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP_mat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpi_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtau\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    105\u001b[0m delta \u001b[38;5;241m=\u001b[39m compute_delta(d, pi_old)\n\u001b[0;32m    106\u001b[0m pol_adv \u001b[38;5;241m=\u001b[39m compute_expected_policy_advantage(rel_pol_adv, d)\n",
      "File \u001b[1;32mc:\\Users\\crist\\OneDrive - Politecnico di Milano\\Polimi\\Tesi\\TMDP\\model_functions.py:251\u001b[0m, in \u001b[0;36mcompute_d_from_tau\u001b[1;34m(mu, P_mat, xi, pi, gamma, tau)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mmatmul(mu, np\u001b[38;5;241m.\u001b[39mtranspose(V))\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m######################### Discounted State Distribution #########################\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    Compute the discounted state distribution as d = (1-gamma) * mu * (I - gamma*(1-tau)*P_mat - gamma*tau*Xi)^-1\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    254\u001b[0m \u001b[38;5;124;03m        - mu (np.ndarray): initial state distribution [nS]\u001b[39;00m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;124;03m        - P_mat (np.ndarray): probability transition function of the original problem [nS, nA, nS]\u001b[39;00m\n\u001b[0;32m    256\u001b[0m \u001b[38;5;124;03m        - xi (np.ndarray): state teleport probability distribution [nS]\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m        - pi (np.ndarray): the given policy [nS, nA]\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;124;03m        - gamma (float): discount factor\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[38;5;124;03m        - tau (float): teleport probability\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;124;03m    return (np.ndarray): the discount state distribution as a vector [nS]\u001b[39;00m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_d_from_tau\u001b[39m(mu, P_mat, xi, pi, gamma, tau):\n\u001b[0;32m    263\u001b[0m     nS, nA \u001b[38;5;241m=\u001b[39m pi\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[1;32mc:\\Users\\crist\\OneDrive - Politecnico di Milano\\Polimi\\Tesi\\TMDP\\model_functions.py:64\u001b[0m, in \u001b[0;36mcompute_transition_kernel\u001b[1;34m(P_mat, pi)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nA):\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m s_prime \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(nS):\n\u001b[1;32m---> 64\u001b[0m             P_sprime_s[s][s_prime] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pi[s, a] \u001b[38;5;241m*\u001b[39m P_mat[s][a][s_prime]\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m P_sprime_s\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tmdp.reset()\n",
    "tmdp.update_tau(.5)\n",
    "cur_res = curriculum_PG(tmdp, policy_pi, opt, episodes=300000, alpha=.25, batch_nS=1, biased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  1.  0. -1.]\n",
      " [ 0.  1. -1.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " ...\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.]\n",
      " [ 1.  0. -1.  0.]]\n",
      "optimal performance:  0.013302794647291147 curriculum performance:  0.0\n"
     ]
    }
   ],
   "source": [
    "res = bellman_optimal_q(tmdp.env.P_mat, tmdp.env.reward, tmdp.gamma)\n",
    "Q = res[\"Q\"]\n",
    "\n",
    "d = compute_d_from_tau(tmdp.env.mu, tmdp.env.P_mat, tmdp.xi, get_policy(Q), tmdp.gamma, 0.)\n",
    "d_curr = compute_d_from_tau(tmdp.env.mu, tmdp.P_mat_tau, tmdp.xi, get_policy(policy_pi.get_probabilities()), tmdp.gamma, 0.)\n",
    "\n",
    "print(get_policy(Q) - get_policy(policy_pi.get_probabilities()))\n",
    "\n",
    "r_s_a = compute_r_s_a(tmdp.env.P_mat, tmdp.env.reward)\n",
    "\n",
    "j_opt = compute_j(r_s_a, get_policy(Q), d, tmdp.gamma)\n",
    "j_curr = compute_j(r_s_a, get_policy(policy_pi.get_probabilities()), d_curr, tmdp.gamma)\n",
    "print(\"optimal performance: \",j_opt, \"curriculum performance: \",j_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.26462815e-03 4.66352560e-03 4.57481569e-03 1.38917348e-07]\n",
      " [1.35067704e-02 2.80340682e-03 6.01054024e-03 5.47126938e-03]\n",
      " [4.05905334e-03 3.09667151e-03 4.59847107e-03 0.00000000e+00]\n",
      " ...\n",
      " [1.36251175e-03 9.90257673e-08 7.50414788e+00 1.00774296e-03]\n",
      " [0.00000000e+00 0.00000000e+00 8.67017216e+00 0.00000000e+00]\n",
      " [4.52946214e-03 5.32592422e-03 3.05887965e-03 3.88646212e-03]]\n"
     ]
    }
   ],
   "source": [
    "Q = cur_res[\"Q\"]\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmdp = TMDP(env, xi_frozen, tau=0., gamma=gamma, seed=seed)\n",
    "pi = get_policy(Q)\n",
    "env.render_mode = \"human\"\n",
    "tmdp.reset()\n",
    "done = False\n",
    "step = 0\n",
    "while True:\n",
    "    a = greedy(tmdp.env.s, pi, tmdp.env.allowed_actions[int(tmdp.env.s)])\n",
    "    s_prime, reward, flags, prob = tmdp.step(a)\n",
    "    if flags[\"done\"]:\n",
    "        tmdp.reset()\n",
    "        break\n",
    "    step +=1\n",
    "    if step > max(100,nrows*2):\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
