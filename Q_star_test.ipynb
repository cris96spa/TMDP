{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal Q_tau* test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "junix --filepath ./Test.ipynb --output_dir ./results/intermediate_curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from TMDP import TMDP\n",
    "from RiverSwimSwimSwim impRiverSwimiverSwim\n",
    "\n",
    "from algorithms import *\n",
    "from model_functions import *\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with tau=0.9\n",
    "nS = 8\n",
    "nA = 2\n",
    "seed = 3231\n",
    "gamma = 0.9\n",
    "river = RiverSwimSwim(nS, gamma, 5, 1000)\n",
    "tau = 0.9\n",
    "status_step = 500\n",
    "xi = np.ones(river.nS)*1/river.nS\n",
    "tmdp = TMDP(river, xi, tau, gamma, seed)\n",
    "taus = [0.90, 0.80, 0.70, 0.60, 0.50, 0.40, 0.30, 0.20, 0.10, 0.090, 0.080, 0.070, 0.060, 0.050, 0.040, 0.030, 0.020, 0.010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import Env, spaces\n",
    "from gym.utils import seeding\n",
    "from model_functions import * \n",
    "\n",
    "taus = np.flip(np.linspace(0, 0.9, num=50))\n",
    "type(np.ones(7))\n",
    "type({})\n",
    "type(spaces.Discrete(nA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "iterations = {}\n",
    "# Learning from scratch with different values of tau\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    \n",
    "    Q_star_tau, iterations[tau] = bellman_optimal_q(tmdp.nS, tmdp.nA, tmdp.P_mat_tau, tmdp.reward, 1e-4, tmdp.gamma)\n",
    "    \n",
    "    d = compute_d(tmdp.mu, tmdp.P_mat_tau, get_policy(Q_star_tau), tmdp.gamma)\n",
    "    # Compute the gamma discounted state distribution\n",
    "    delta = compute_delta(d, get_policy(Q_star_tau))\n",
    "    # Compute the state value function\n",
    "    V_star_tau = get_value_function(Q_star_tau) \n",
    "    # Compute the expected reward when picking action a in state s\n",
    "    r_s_a = compute_r_s_a(nS, nA, tmdp.P_mat_tau, tmdp.reward)\n",
    "    # Compute the state action next-state value function U_tau(s,a,s') = R(s,a) + \\gamma*V_tau(s')\n",
    "    U_star_tau = compute_state_action_nextstate_value_function(nS, nA, r_s_a, tmdp.gamma, V_star_tau)\n",
    "    # Rebuild Q using U as Q_tau(s,a) = \\sum{s' \\in S}P_tau(s'|s,a)*U_tau(s,a,s')\n",
    "    Q_t = compute_Q_from_U(tmdp.P_mat_tau, U_star_tau)\n",
    "\n",
    "    # Compute the relative model advantage function hat \\hat{A}_{tau, mu}(s,a)\n",
    "    A_tau_hat = compute_relative_model_advantage_function_hat(tmdp.P_mat, tmdp.xi, U_star_tau)\n",
    "    # Compute the discounted distribution relative model advantage function hat \\hat{A}_{tau, mu}\n",
    "    A_hat = compute_discounted_distribution_relative_model_advantage_function_hat(A_tau_hat, delta)\n",
    "    # The dissimilarity term D = D_e * gamma * D_inf is upperbounded by 4*gamma+(tau - tau_1)\n",
    "    # Compute Delta Q_tau as the superior among the difference of the L_1 norm of elements of Q_tau\n",
    "    d_q = get_sup_difference_Q(Q_star_tau)\n",
    "    tau_1 = compute_tau_prime(A_hat, tmdp.gamma, tmdp.tau, d_q)\n",
    "    \n",
    "    J_star_tau = get_expected_avg_reward(tmdp.P_mat_tau, get_policy(Q_star_tau), tmdp.reward, tmdp.gamma, tmdp.mu)\n",
    "    J_0 = get_expected_avg_reward(tmdp.P_mat, get_policy(Q_star_tau), tmdp.reward, tmdp.gamma, tmdp.mu)\n",
    "\n",
    "    # Compute the performance improvement lower bound when moving to tau=0\n",
    "    l_b = compute_performance_improvement_lower_bound(A_hat, tmdp.gamma, d_q, tmdp.tau, 0.0)\n",
    "    print(\"Moving from {} to {}\".format(tmdp.tau, 0.))\n",
    "    print(\"Theoretical lb on performance improvement: {}, Empirical one: {}\".format(l_b, J_0-J_star_tau))\n",
    "    print(\"Optimal tau': \", tau_1)\n",
    "    print(\"Advantage: \", (tmdp.tau*A_hat))\n",
    "    print(\"\\n\")\n",
    "    results.append({\"Q\": Q_star_tau, \"tau\":tmdp.tau, \"J\":J_star_tau, \"J_0\":J_0, \"policy\":get_policy(Q_star_tau), \"l_b\":l_b, \"Adv\":tmdp.tau*A_hat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "z = np.array([])\n",
    "for tau in taus:\n",
    "    y = np.append(y, iterations[tau])\n",
    "    label = \"τ=\"+str(tau)\n",
    "    x = np.append(x, label)\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "#fig, ax = plt.subplots(fignS=(5, 2.7), layout='constrained')\n",
    "plt.scatter(x, y, c='orange', label='#iterations for convergence')\n",
    "plt.xticks(x_map, x)\n",
    "\n",
    "plt.xlabel(\"Current value of τ\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"E[J(τ)]\")\n",
    "plt.title(\"Expected discounted sum of rewards\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "k = np.array([])\n",
    "for i, res in enumerate(results):\n",
    "    if i < len(results)-1:\n",
    "        y = np.append(y, res['J'])\n",
    "        label = \"τ=\"+str(res['tau'])\n",
    "        x = np.append(x, label)\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "#fig, ax = plt.subplots(fignS=(5, 2.7), layout='constrained')\n",
    "plt.scatter(x, y, c='orange', label='#iterations')\n",
    "plt.xticks(x_map, x)\n",
    "\n",
    "plt.hlines(y=results[-1]['J_0'], xmin=0, xmax=len(x), colors='g')\n",
    "plt.xlabel(\"Current value of τ\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"E[J(τ)]\")\n",
    "plt.title(\"Expected discounted sum of rewards\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "z = np.array([])\n",
    "k = np.array([])\n",
    "old = -np.inf\n",
    "for i, res in enumerate(results):\n",
    "    if i < len(results)-1:\n",
    "        y = np.append(y, res['J'])\n",
    "        label = \"τ=\"+str(res['tau'])\n",
    "        x = np.append(x, label)\n",
    "        z = np.append(z, res['J_0'])\n",
    "        k = np.append(k, res['Adv'])\n",
    "        if res['Adv'] > 0 and old < 0:\n",
    "            t_lim = res['tau']\n",
    "            old = np.inf\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "#fig, ax = plt.subplots(fignS=(5, 2.7), layout='constrained')\n",
    "#plt.scatter(x, y, c='orange', label='J on current τ')\n",
    "#plt.scatter(x, z, label='J on original problem')\n",
    "plt.scatter(x, k, label='Adv', c='r')\n",
    "plt.xticks(x_map, x)\n",
    "\n",
    "plt.hlines(y=0, xmin=0, xmax=len(x), colors='g')\n",
    "plt.xlabel(\"Current value of τ\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"E[J(τ)]\")\n",
    "plt.title(\"Expected discounted sum of rewards\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The limit value of tau is:\", t_lim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "z = np.array([])\n",
    "for i, res in enumerate(results):\n",
    "    if i < len(results)-1:\n",
    "        y = np.append(y, res['J_0'] - res['J'])\n",
    "        label = str(res['tau'])\n",
    "        x = np.append(x, label)\n",
    "        z = np.append(z, res['l_b'])\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "\n",
    "plt.scatter(x, y, label='J_0 - J_τ')\n",
    "plt.scatter(x, z,  c='orange', label='Performance improvement lower bound')\n",
    "plt.xticks(x_map, x)\n",
    "\n",
    "plt.xlabel(\"Current value of τ\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"E[J(τ)]\")\n",
    "plt.title(\"Performance improvement\")\n",
    "\n",
    "plt.legend(loc=\"lower right\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# Learning from scratch with different values of tau\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    \n",
    "    Q_star_tau = bellman_optimal_q(tmdp.nS, tmdp.nA, tmdp.P_mat_tau, tmdp.reward, 1e-4, tmdp.gamma)\n",
    "\n",
    "    d = compute_d(tmdp.mu, tmdp.P_mat_tau, get_policy(Q_star_tau), tmdp.gamma)\n",
    "    # Compute the gamma discounted state distribution\n",
    "    delta = compute_delta(d, get_policy(Q_star_tau))\n",
    "    # Compute the state value function\n",
    "    V_star_tau = get_value_function(Q_star_tau) \n",
    "    # Compute the expected reward when picking action a in state s\n",
    "    r_s_a = compute_r_s_a(nS, nA, tmdp.P_mat_tau, tmdp.reward)\n",
    "    # Compute the state action next-state value function U_tau(s,a,s') = R(s,a) + \\gamma*V_tau(s')\n",
    "    U_star_tau = compute_state_action_nextstate_value_function(nS, nA, r_s_a, tmdp.gamma, V_star_tau)\n",
    "    # Rebuild Q using U as Q_tau(s,a) = \\sum{s' \\in S}P_tau(s'|s,a)*U_tau(s,a,s')\n",
    "    Q_t = compute_Q_from_U(tmdp.P_mat_tau, U_star_tau)\n",
    "\n",
    "\n",
    "    # Compute the relative model advantage function hat \\hat{A}_{tau, mu}(s,a)\n",
    "    A_tau_hat = compute_relative_model_advantage_function_hat(tmdp.P_mat, tmdp.xi, U_star_tau)\n",
    "    # Compute the discounted distribution relative model advantage function hat \\hat{A}_{tau, mu}\n",
    "    A_hat = compute_discounted_distribution_relative_model_advantage_function_hat(A_tau_hat, delta)\n",
    "    # The dissimilarity term D = D_e * gamma * D_inf is upperbounded by 4*gamma+(tau - tau_1)\n",
    "    # Compute Delta Q_tau as the superior among the difference of the L_1 norm of elements of Q_tau\n",
    "    d_q = get_sup_difference_Q(Q_star_tau)\n",
    "    \n",
    "    # Compute the performance improvement lower bound when moving to tau=0\n",
    "    l_b = compute_performance_improvement_lower_bound(A_hat, tmdp.gamma, d_q, tmdp.tau, 0.0)\n",
    "    \n",
    "    \n",
    "    tau_1 = compute_tau_prime(A_hat, tmdp.gamma, tmdp.tau, d_q)\n",
    "    tmdp_1 = tmdp = TMDP(river, xi, tau_1, gamma)\n",
    "    print(tmdp.tau)\n",
    "   \n",
    "    print(tmdp_1.tau)\n",
    "    print(tmdp.tau - A_hat*(1-gamma)/(4*gamma**2*d_q))\n",
    "\n",
    "    J_star_tau = get_expected_avg_reward(tmdp.P_mat_tau, get_policy(Q_star_tau), tmdp.reward, tmdp.gamma, tmdp.mu)\n",
    "    J_0 = get_expected_avg_reward(tmdp.P_mat, get_policy(Q_star_tau), tmdp.reward, tmdp.gamma, tmdp.mu)\n",
    "    J_1 = get_expected_avg_reward(tmdp_1.P_mat_tau, get_policy(Q_star_tau), tmdp_1.reward, tmdp_1.gamma, tmdp_1.mu)\n",
    "\n",
    "    print(tmdp.tau)\n",
    "   \n",
    "\n",
    "    print(\"Moving from {} to {}\".format(tmdp.tau, 0.))\n",
    "    print(\"Theoretical lb on performance improvement: {}, Empirical one: {}\\n\".format(l_b, J_0-J_star_tau))\n",
    "\n",
    "    print(\"Moving from {} to {}\".format(tmdp.tau, tmdp_1.tau))\n",
    "    l_b_1 = compute_performance_improvement_lower_bound(A_hat, tmdp.gamma, d_q, tmdp.tau, tau_1)\n",
    "    opt_lb = compute_optimal_lower_bound(A_hat, gamma, d_q)\n",
    "    print(\"Theoretical lb on performance improvement: {}, Empirical one: {}\".format(l_b_1, J_1-J_star_tau))\n",
    "    print(\"\\nThe lower bound for tau' is: {}\".format(opt_lb))\n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    results.append({\"Q\": Q_star_tau, \"tau\":tmdp.tau, \"J\":J_star_tau, \"J_0\":J_0, \"policy\":get_policy(Q_star_tau), \"l_b\":l_b})\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "bad = []\n",
    "iterations = {}\n",
    "# Learning from scratch with different values of tau\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    \n",
    "    Q_star_tau, iterations[tau] = bellman_optimal_q(tmdp.nS, tmdp.nA, tmdp.P_mat_tau, tmdp.reward, 1e-4, tmdp.gamma)\n",
    "\n",
    "    d = compute_d(tmdp.mu, tmdp.P_mat_tau, get_policy(Q_star_tau), tmdp.gamma)\n",
    "    # Compute the gamma discounted state distribution\n",
    "    delta = compute_delta(d, get_policy(Q_star_tau))\n",
    "    # Compute the state value function\n",
    "    V_star_tau = get_value_function(Q_star_tau) \n",
    "    # Compute the expected reward when picking action a in state s\n",
    "    r_s_a = compute_r_s_a(nS, nA, tmdp.P_mat_tau, tmdp.reward)\n",
    "    # Compute the state action next-state value function U_tau(s,a,s') = R(s,a) + \\gamma*V_tau(s')\n",
    "    U_star_tau = compute_state_action_nextstate_value_function(nS, nA, r_s_a, tmdp.gamma, V_star_tau)\n",
    "    # Rebuild Q using U as Q_tau(s,a) = \\sum{s' \\in S}P_tau(s'|s,a)*U_tau(s,a,s')\n",
    "    Q_t = compute_Q_from_U(tmdp.P_mat_tau, U_star_tau)\n",
    "\n",
    "\n",
    "    # Compute the relative model advantage function hat \\hat{A}_{tau, mu}(s,a)\n",
    "    A_tau_hat = compute_relative_model_advantage_function_hat(tmdp.P_mat, tmdp.xi, U_star_tau)\n",
    "    # Compute the discounted distribution relative model advantage function hat \\hat{A}_{tau, mu}\n",
    "    A_hat = compute_discounted_distribution_relative_model_advantage_function_hat(A_tau_hat, delta)\n",
    "    # The dissimilarity term D = D_e * gamma * D_inf is upperbounded by 4*gamma+(tau - tau_1)\n",
    "    # Compute Delta Q_tau as the superior among the difference of the L_1 norm of elements of Q_tau\n",
    "    d_q = get_sup_difference_Q(Q_star_tau)\n",
    "    \n",
    "    J_star_tau = get_expected_avg_reward(tmdp.P_mat_tau, get_policy(Q_star_tau), tmdp.reward, tmdp.gamma, tmdp.mu)\n",
    "    print(\"evaluating for tau: {}\".format(tmdp.tau))\n",
    "    for t in taus:\n",
    "        if(t < tau):\n",
    "            tmdp_1 = TMDP(river, xi, t, gamma)\n",
    "            J_1 = get_expected_avg_reward(tmdp_1.P_mat_tau , get_policy(Q_star_tau), tmdp.reward, tmdp.gamma, tmdp.mu)\n",
    "\n",
    "            # Compute the performance improvement lower bound when moving to tau=0\n",
    "            l_b = compute_performance_improvement_lower_bound(A_hat, tmdp.gamma, d_q, tmdp.tau, tmdp_1.tau)\n",
    "            print(\"Moving from {} to {}\".format(tmdp.tau, tmdp_1.tau))\n",
    "            print(\"Theoretical lb on performance improvement: {}, Empirical one: {}\\n\".format(l_b, J_1-J_star_tau))\n",
    "            if J_1-J_star_tau < l_b:\n",
    "                bad.append({'tau':tmdp.tau, 'tau_1':tmdp_1.tau, \"J_star\":J_star_tau, \"J_1\":J_1})\n",
    "    print(\"\\n\")\n",
    "    results.append({\"Q\": Q_star_tau, \"tau\":tmdp.tau, \"J\":J_star_tau, \"J_0\":J_0, \"policy\":get_policy(Q_star_tau), \"l_b\":l_b})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
