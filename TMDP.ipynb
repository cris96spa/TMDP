{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from TMDP import TMDP\n",
    "from RiverSwimSwim import RiverSwim\n",
    "\n",
    "from algorithms import *\n",
    "from model_functions import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#np.set_printoptions(precision=4)\n",
    "import math\n",
    "from utils import *\n",
    "\n",
    "nS = 8\n",
    "nA = 2\n",
    "seed = 2184109\n",
    "gamma = .9\n",
    "mu = np.ones(nS) * 1/nS\n",
    "river = RiverSwim(nS, mu, gamma=gamma, small=5, large=1000, seed=seed)\n",
    "tau = 0.3\n",
    "xi = np.ones(nS) * 1/nS\n",
    "tmdp = TMDP(river, xi, tau=tau, gamma=gamma, seed=seed)\n",
    "mdp = TMDP(river, xi, tau=0., gamma=gamma, seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_s_a = compute_r_s_a(tmdp.P_mat_tau, tmdp.reward)\n",
    "r_s_a_xi = compute_r_s_a(tmdp.xi, tmdp.reward)\n",
    "r_s_a_p = compute_r_s_a(tmdp.P_mat, tmdp.reward)\n",
    "\n",
    "Q_star, _ = bellman_optimal_q(tmdp.P_mat_tau, tmdp.reward, gamma).values()\n",
    "pi = get_policy(Q_star)\n",
    "pi_prime = np.array([[0., 1.] for i in range(nS)])\n",
    "d = compute_d(mu, tmdp.P_mat_tau, pi, tmdp.gamma)\n",
    "V_star = get_value_function(Q_star)\n",
    "print(tmdp.P_mat.shape)\n",
    "print(Q_star)\n",
    "\n",
    "Q_star_0 , _ = bellman_optimal_q(tmdp.P_mat, tmdp.reward, gamma).values()\n",
    "print(Q_star_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r_s_a_p)\n",
    "print(r_s_a_xi)\n",
    "print(r_s_a)\n",
    "\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_step = 40000\n",
    "episodes = 1000000\n",
    "q_p = get_q_hat( tmdp.P_mat, r_s_a_p, tmdp.gamma, Q_star)\n",
    "q_xi = get_q_hat(tmdp.xi, r_s_a_xi, tmdp.gamma, Q_star)\n",
    "\n",
    "print(\"Q_hat_P:\\n\", q_p)\n",
    "print(\"Q_hat_xi:\\n\",q_xi)\n",
    "print(\"Rebuilted Q:\\n\", (1-tmdp.tau) * q_p + tmdp.tau * q_xi)\n",
    "print(\"Q:\\n\", Q_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((nS, nA))\n",
    "Qs = Q_learning(tmdp, Q, alpha=1., episodes=episodes, status_step=status_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_0 = np.zeros((nS, nA))\n",
    "Qs_0 = Q_learning(mdp, Q_0, alpha=1., episodes=episodes, status_step=status_step, state_distribution=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_policy(Qs[\"Qs\"][-1]))\n",
    "print(get_policy(Q_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = Qs[\"Qs\"][-1]\n",
    "print(Q)\n",
    "\n",
    "q_p = get_q_hat( tmdp.P_mat, r_s_a_p, tmdp.gamma, Q)\n",
    "q_xi = get_q_hat(tmdp.xi, r_s_a_xi, tmdp.gamma, Q)\n",
    "\n",
    "print(\"Q_hat_P:\\n\", q_p)\n",
    "print(\"Q_hat_xi:\\n\",q_xi)\n",
    "print(\"Rebuilted Q:\\n\", (1-tmdp.tau) * q_p + tmdp.tau * q_xi)\n",
    "print(\"Q:\\n\", Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Qs[\"Qs\"][-1])\n",
    "print(Qs_0[\"Qs\"][-1])\n",
    "visits = Qs[\"visits\"]\n",
    "print(visits)\n",
    "visit_dist = visits / np.sum(visits)\n",
    "print(visit_dist)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visit_weights = 1.0 / (d + 1e-8)\n",
    "visit_weights = visit_weights / np.sum(visit_weights)\n",
    "print(visit_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau_prime = 0.\n",
    "metrics = compute_metrics(tmdp, Qs[\"Qs\"], Q_star, Qs[\"visit_distributions\"], tau_prime=tau_prime)\n",
    "metrics_0 = compute_metrics(mdp, Qs_0[\"Qs\"], Q_star_0, Qs_0[\"visit_distributions\"], tau_prime=tau_prime)\n",
    "print(metrics)\n",
    "print(metrics_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y_0 = np.array([])\n",
    "y = np.array([])\n",
    "\n",
    "for i, J in enumerate(metrics[\"J\"]):\n",
    "    y = np.append(y, J)\n",
    "    y_0 = np.append(y_0, metrics_0[\"J\"][i])\n",
    "    if i < len(metrics[\"J\"])-1:\n",
    "        x = np.append(x, (i+1)*status_step)\n",
    "    else:\n",
    "        x = np.append(x, \"Q*\")\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "plt.scatter(x, y, c=\"orange\")\n",
    "plt.scatter(x, y_0, c=\"blue\")\n",
    "\n",
    "plt.plot(x[:-1], y[:-1], c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.plot(x[:-1], y_0[:-1], c=\"blue\", label=\"τ=0\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"E[J]\")\n",
    "plt.title(\"Expected discounted sum of rewards J(τ=0)\")\n",
    "plt.grid(visible=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y_0 = np.array([])\n",
    "y = np.array([])\n",
    "\n",
    "for i, J in enumerate(metrics[\"grad_J\"]):\n",
    "    y = np.append(y, J)\n",
    "    y_0 = np.append(y_0, metrics_0[\"grad_J\"][i])\n",
    "    if i < len(metrics[\"grad_J\"])-1:\n",
    "        x = np.append(x, (i+1)*status_step)\n",
    "    else:\n",
    "        x = np.append(x, \"Q*\")\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "plt.scatter(x, y, c=\"orange\")\n",
    "plt.scatter(x, y_0, c=\"blue\")\n",
    "\n",
    "plt.plot(x[:-1], y[:-1], c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.plot(x[:-1], y_0[:-1], c=\"blue\", label=\"τ=0\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"∇_τ J\")\n",
    "plt.title(\"Gradient of the expected discounted sum of rewards\")\n",
    "plt.grid(visible=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "for i, J in enumerate(metrics[\"J_tau\"]):\n",
    "    y = np.append(y, J)\n",
    "    if i < len(metrics[\"J_tau\"])-1:\n",
    "        x = np.append(x, (i+1)*status_step)\n",
    "    else:\n",
    "        x = np.append(x, \"Q*\")\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "plt.xticks(x_map, x)\n",
    "plt.scatter(x, y, c=\"orange\")\n",
    "\n",
    "plt.plot(x[:-1], y[:-1], c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"E[J(τ)]\")\n",
    "plt.title(\"Expected discounted sum of rewards (τ={})\".format(tmdp.tau))\n",
    "plt.grid(visible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "for i, J in enumerate(metrics[\"delta_J\"]):\n",
    "    y = np.append(y, J)\n",
    "    if i < len(metrics[\"delta_J\"])-1:\n",
    "        x = np.append(x, (i+1)*status_step)\n",
    "    else:\n",
    "        x = np.append(x, \"Q*\")\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "plt.xticks(x_map, x)\n",
    "plt.scatter(x, y, c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.plot(x[:-1], y[:-1], c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"J- J(τ={}).\".format(tmdp.tau))\n",
    "plt.title(\"Difference between expected discounted sum of rewards J and J(τ={})\".format(tmdp.tau))\n",
    "plt.grid(visible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "y_0 = np.array([])\n",
    "for i, J in enumerate(metrics[\"delta_Q\"]):\n",
    "    y = np.append(y, J)\n",
    "    y_0 = np.append(y_0, metrics_0[\"delta_Q\"][i])\n",
    "    if i < len(metrics[\"delta_Q\"])-1:\n",
    "        x = np.append(x, (i+1)*status_step)\n",
    "    else:\n",
    "        x = np.append(x, \"Q*\")\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "plt.scatter(x, y, c=\"orange\")\n",
    "plt.scatter(x, y_0, c=\"blue\")\n",
    "\n",
    "plt.plot(x[:-1], y[:-1], c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.plot(x[:-1], y_0[:-1], c=\"blue\", label=\"τ=0\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"L_inf|Q(τ) - Q*|\")\n",
    "plt.title(\"L_infinity norm of |Q(τ) - Q*|\")\n",
    "plt.grid(visible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "for i, J in enumerate(metrics[\"l_bounds\"]):\n",
    "    y = np.append(y, J)\n",
    "    if i < len(metrics[\"l_bounds\"])-1:\n",
    "        x = np.append(x, (i+1)*status_step)\n",
    "    else:\n",
    "        x = np.append(x, \"Q*\")\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "plt.scatter(x, y, c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.plot(x[:-1], y[:-1], c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"J(0) - J(τ) lower bound\")\n",
    "plt.title(\"Performance Improvement Lower Bound\")\n",
    "plt.grid(visible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "for i, J in enumerate(metrics[\"adv_terms\"]):\n",
    "    y = np.append(y, J)\n",
    "    if i < len(metrics[\"adv_terms\"])-1:\n",
    "        x = np.append(x, (i+1)*status_step)\n",
    "    else:\n",
    "        x = np.append(x, \"Q*\")\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "plt.scatter(x, y, c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.plot(x[:-1], y[:-1], c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Advantage\")\n",
    "plt.title(\"Advantage in moving from τ={} to τ={}\".format(tmdp.tau, tau_prime))\n",
    "plt.grid(visible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([])\n",
    "y = np.array([])\n",
    "for i, J in enumerate(metrics[\"diss_terms\"]):\n",
    "    y = np.append(y, J)\n",
    "    if i < len(metrics[\"diss_terms\"])-1:\n",
    "        x = np.append(x, (i+1)*status_step)\n",
    "    else:\n",
    "        x = np.append(x, \"Q*\")\n",
    "x_map = np.array([i for i in range(len(x))])\n",
    "plt.scatter(x, y, c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.plot(x[:-1], y[:-1], c=\"orange\", label=\"τ={}\".format(tmdp.tau))\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel(\"Dissimilarity Penalization\")\n",
    "plt.title(\"Dissimilarity penalization in moving from τ={} to τ={}\".format(tmdp.tau, tau_prime))\n",
    "plt.grid(visible=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Q_star_0)\n",
    "print(r_s_a_p)\n",
    "print(r_s_a_xi)\n",
    "\n",
    "pi_0 = get_policy(Q_star_0)\n",
    "print(pi_0)\n",
    "Q_xi_0 = get_q_hat(mdp.xi, r_s_a_xi, mdp.gamma, Q_star_0)\n",
    "Q_p_0 = get_q_hat(mdp.P_mat, r_s_a_p, mdp.gamma, Q_star_0)\n",
    "print(Q_xi_0)\n",
    "print(Q_p_0)\n",
    "\n",
    "compute_grad_j(pi_0, Q_p_0, Q_xi_0, d, mdp.gamma)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
