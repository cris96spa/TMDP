{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimare Q function ottimale\n",
    "## 1. Quanto tempo ci mette per convergere a Q*, al variare di tau\n",
    "## 2. Distanza tra Q* e Q_t \n",
    "## 3. distanza tra Q_0 e Q_0 appresa tramite i valori di tau\n",
    "## 4. cercare di integrare i bound trovati"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrarre policy da Q\n",
    "### Confrontare due diverse Q valutando diverse metriche \n",
    "### Calcolare Q*\n",
    "### Tentativo di implementazione di un curriculum\n",
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from TMDP import TMDP\n",
    "from river_swim import River\n",
    "\n",
    "from algorithms import Q_learning, eps_greedy, SARSA, get_policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with tau=0.9\n",
    "gamma = 0.9\n",
    "river = River(gamma)\n",
    "tau = 0.9\n",
    "xi = np.ones(river.nS)*1/river.nS\n",
    "tmdp = TMDP(river, xi, tau, gamma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1768.52212473 1987.17893525]\n",
      " [1963.25270449 1624.95773134]\n",
      " [1937.31679031 1508.82748886]\n",
      " [1891.02653819 1653.37328062]\n",
      " [2099.89057812 1693.41518147]\n",
      " [1462.20371342 3077.82788508]]\n",
      "[1, 0, 0, 0, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "s = tmdp.reset()\n",
    "M = 1000\n",
    "Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "ret = 0\n",
    "\n",
    "a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "\n",
    "Q = Q_learning(tmdp, s, a, Q, M)\n",
    "\n",
    "print(Q)\n",
    "print(get_policy(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 753.62066833 1463.90531677]\n",
      " [ 877.75311103 2006.88583723]\n",
      " [1605.71172688 3073.57489596]\n",
      " [2554.52462858 4388.79396789]\n",
      " [3906.9081197  6596.53956963]\n",
      " [5323.00627262 9674.65102056]]\n"
     ]
    }
   ],
   "source": [
    "tmdp_0 = TMDP(river, xi, 0, gamma)\n",
    "s = tmdp_0.reset()\n",
    "M = 1000000\n",
    "Q = np.zeros((tmdp_0.nS, tmdp_0.nA))\n",
    "ret = 0\n",
    "\n",
    "a = eps_greedy(s, Q, 1., tmdp_0.allowed_actions[s.item()])\n",
    "\n",
    "Q = Q_learning(tmdp_0, s, a, Q, M)\n",
    "\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[50.         42.30648738]\n",
      " [45.         36.507238  ]\n",
      " [40.4828096  26.80605896]\n",
      " [14.2275224   0.        ]\n",
      " [11.91503429  0.        ]\n",
      " [ 0.          0.        ]]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "pi = get_policy(Q)\n",
    "tmdp_0 = TMDP(river, xi, 0, gamma)\n",
    "s = tmdp_0.reset()\n",
    "M = 1000\n",
    "Q = np.zeros((tmdp_0.nS, tmdp_0.nA))\n",
    "ret = 0\n",
    "\n",
    "a = eps_greedy(s, Q, 1., tmdp_0.allowed_actions[s.item()])\n",
    "\n",
    "Q = Q_learning(tmdp_0, s, a, Q, M)\n",
    "\n",
    "print(Q)\n",
    "print(pi)\n",
    "print(get_policy(Q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[5.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]\n",
      "  [0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]]\n",
      "\n",
      " [[0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]\n",
      "  [0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]]\n",
      "\n",
      " [[0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]\n",
      "  [0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]]\n",
      "\n",
      " [[0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]\n",
      "  [0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]]\n",
      "\n",
      " [[0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]\n",
      "  [0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]]\n",
      "\n",
      " [[0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 0.e+00]\n",
      "  [0.e+00 0.e+00 0.e+00 0.e+00 0.e+00 1.e+04]]]\n"
     ]
    }
   ],
   "source": [
    "print(tmdp.reward)\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' taus = [1 - i*0.1 for i in range(10)]\\ntaus.append(0)\\nQs = []\\nxi = np.ones(river.nS)*1/river.nS\\ngamma = 0.9\\nriver = River(gamma)\\n\\nfor tau in taus:\\n    tmdp = TMDP(river, xi, tau, gamma)\\n    s = tmdp.reset()\\n    M = 10000\\n    Q = np.zeros((tmdp.nS, tmdp.nA))\\n    ret = 0\\n    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\\n    Q = Q_learning(tmdp, s, a, Q, M)\\n    Qs.append({\"tau\":tau, \"Q_function\":Q, \"env\":tmdp})\\n\\nfor i in range(len(taus)):\\n    print(\"Tau:\", Qs[i][\\'tau\\'])\\n    print(Qs[i][\\'Q_function\\']) '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" taus = [1 - i*0.1 for i in range(10)]\n",
    "taus.append(0)\n",
    "Qs = []\n",
    "xi = np.ones(river.nS)*1/river.nS\n",
    "gamma = 0.9\n",
    "river = River(gamma)\n",
    "\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    s = tmdp.reset()\n",
    "    M = 10000\n",
    "    Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "    ret = 0\n",
    "    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "    Q = Q_learning(tmdp, s, a, Q, M)\n",
    "    Qs.append({\"tau\":tau, \"Q_function\":Q, \"env\":tmdp})\n",
    "\n",
    "for i in range(len(taus)):\n",
    "    print(\"Tau:\", Qs[i]['tau'])\n",
    "    print(Qs[i]['Q_function']) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" pi = get_policy(Qs[-2]['Q_function'])\\npi_prime = get_policy(Qs[-1]['Q_function'])\\n\\nprint(pi)\\nprint(pi_prime) \""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" pi = get_policy(Qs[-2]['Q_function'])\n",
    "pi_prime = get_policy(Qs[-1]['Q_function'])\n",
    "\n",
    "print(pi)\n",
    "print(pi_prime) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q function learned from scratch:\n",
      " [[50.         43.34466023]\n",
      " [45.         38.42221949]\n",
      " [40.5        36.17559844]\n",
      " [36.44999997 32.32161598]\n",
      " [31.04537719  0.        ]\n",
      " [ 0.          0.        ]]\n",
      "Q function learned with transfer learning:\n",
      " [[ 605.81142348 1376.80434816]\n",
      " [ 825.33973966 1759.68131751]\n",
      " [1477.44893237 2800.16732653]\n",
      " [2125.69256981 3811.57571264]\n",
      " [3743.96004087 6017.86800076]\n",
      " [5616.43663002 8931.7819449 ]]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "tmdp = TMDP(river, xi, 1, gamma)\n",
    "Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "\n",
    "# Curriculul for decreasing values of tau\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    s = tmdp.reset()\n",
    "    M = 10000\n",
    "    if tau == 0:\n",
    "        M = 100000\n",
    "    ret = 0\n",
    "    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "    Q = Q_learning(tmdp, s, a, Q, M)\n",
    "\n",
    "\n",
    "print(\"Q function learned from scratch:\\n\", Qs[-1]['Q_function'])\n",
    "print(\"Q function learned with transfer learning:\\n\", Q)\n",
    "\n",
    "pi = get_policy(Qs[-1]['Q_function'])\n",
    "pi_prime = get_policy(Q)\n",
    "\n",
    "print(pi)\n",
    "print(pi_prime)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
