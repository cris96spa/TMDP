{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimare Q function ottimale\n",
    "## 1. Quanto tempo ci mette per convergere a Q*, al variare di tau\n",
    "## 2. Distanza tra Q* e Q_t \n",
    "## 3. distanza tra Q_0 e Q_0 appresa tramite i valori di tau\n",
    "## 4. cercare di integrare i bound trovati"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrarre policy da Q\n",
    "### Confrontare due diverse Q valutando diverse metriche \n",
    "### Calcolare Q*\n",
    "### Tentativo di implementazione di un curriculum\n",
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from TMDP import TMDP\n",
    "from river_swim import River\n",
    "\n",
    "from algorithms import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with tau=0.9\n",
    "nS = 8\n",
    "gamma = 0.9\n",
    "river = River(nS, gamma, 5, 1000)\n",
    "tau = 0.9\n",
    "xi = np.ones(river.nS)*1/river.nS\n",
    "tmdp = TMDP(river, xi, tau, gamma)\n",
    "tmdp_0 = TMDP(river, xi, 0, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q function with tau: 0.9\n",
      "\n",
      "[[104.8042924  152.52889372]\n",
      " [108.03522441 152.96243176]\n",
      " [153.71736149 120.40499111]\n",
      " [150.89024271  95.43266164]\n",
      " [ 97.87452594 151.51808776]\n",
      " [150.79477754 107.3520062 ]\n",
      " [150.34058906 115.51313149]\n",
      " [100.8315816  246.76945879]]\n",
      "[1, 1, 0, 0, 1, 0, 0, 1]\n",
      "Q function with tau: 0\n",
      "[[50.         43.57353625]\n",
      " [45.         38.0638219 ]\n",
      " [40.5        35.74364593]\n",
      " [36.45       29.79083485]\n",
      " [32.805      29.47719167]\n",
      " [29.52302679  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " Q function with bellman operator\n",
      "[[ 64.33978241  66.4264707 ]\n",
      " [ 59.78382363  92.64863548]\n",
      " [ 83.38377194 137.08716883]\n",
      " [123.37845195 203.98538599]\n",
      " [183.58684739 303.06311755]\n",
      " [272.75680579 449.4704471 ]\n",
      " [404.52340239 665.78575189]\n",
      " [599.2071767  985.43074649]]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "s = tmdp.reset()\n",
    "M = 10000\n",
    "Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "ret = 0\n",
    "a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "Q = Q_learning(tmdp, s, a, Q, M)\n",
    "\n",
    "print(\"\\nQ function with tau: {}\\n\".format(tmdp.tau))\n",
    "print(Q)\n",
    "print(get_policy(Q))\n",
    "\n",
    "s = tmdp_0.reset()\n",
    "M_0 = 10000000\n",
    "Q = np.zeros((tmdp_0.nS, tmdp_0.nA))\n",
    "ret = 0\n",
    "a = eps_greedy(s, Q, 1., tmdp_0.allowed_actions[s.item()])\n",
    "Q = Q_learning(tmdp_0, s, a, Q, M_0)\n",
    "\n",
    "\n",
    "print(\"Q function with tau: {}\".format(tmdp_0.tau))\n",
    "print(Q)\n",
    "print(get_policy(Q))\n",
    "\n",
    "r_s_a = compute_r_s_a(tmdp_0.nS, tmdp_0.nA, tmdp_0.P_mat, tmdp_0.reward)\n",
    "q_star = bellman_optimal_q(tmdp_0.nS, tmdp_0.nA, tmdp_0.P_mat, tmdp_0.reward, 1, gamma)\n",
    "\n",
    "\n",
    "print(\"\\n Q function with bellman operator\")\n",
    "print(q_star)\n",
    "print(get_policy(q_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "985.430746492539\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1584.6379231962903"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(compute_delta_q(q_star, Q))\n",
    "np.linalg.norm(np.abs(Q-q_star), np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xi = np.ones(river.nS)*1/river.nS\\ngamma = 0.9\\nriver = River(gamma)\\n\\nfor tau in taus:\\n    tmdp = TMDP(river, xi, tau, gamma)\\n    s = tmdp.reset()\\n    M = 10000\\n    Q = np.zeros((tmdp.nS, tmdp.nA))\\n    ret = 0\\n    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\\n    Q = Q_learning(tmdp, s, a, Q, M)\\n    Qs.append({\"tau\":tau, \"Q_function\":Q, \"env\":tmdp})\\n\\nfor i in range(len(taus)):\\n    print(\"Tau:\", Qs[i][\\'tau\\'])\\n    print(Qs[i][\\'Q_function\\']) '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" xi = np.ones(river.nS)*1/river.nS\n",
    "gamma = 0.9\n",
    "river = River(gamma)\n",
    "\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    s = tmdp.reset()\n",
    "    M = 10000\n",
    "    Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "    ret = 0\n",
    "    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "    Q = Q_learning(tmdp, s, a, Q, M)\n",
    "    Qs.append({\"tau\":tau, \"Q_function\":Q, \"env\":tmdp})\n",
    "\n",
    "for i in range(len(taus)):\n",
    "    print(\"Tau:\", Qs[i]['tau'])\n",
    "    print(Qs[i]['Q_function']) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" pi = get_policy(Qs[-2]['Q_function'])\\npi_prime = get_policy(Qs[-1]['Q_function'])\\n\\nprint(pi)\\nprint(pi_prime) \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" pi = get_policy(Qs[-2]['Q_function'])\n",
    "pi_prime = get_policy(Qs[-1]['Q_function'])\n",
    "\n",
    "print(pi)\n",
    "print(pi_prime) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q function learned with transfer learning:\n",
      " [[  50.           52.05508102]\n",
      " [  45.32535716   67.44292293]\n",
      " [  61.00966625  100.32531515]\n",
      " [  89.98939402  163.74387092]\n",
      " [ 139.44737939  261.51897406]\n",
      " [ 211.03520929  474.29833638]\n",
      " [ 321.49348673  704.94195898]\n",
      " [ 446.08854619 1038.11073228]]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "taus = [1 - i*0.1 for i in range(10)]\n",
    "taus.append(0)\n",
    "\n",
    "# Curriculul for decreasing values of tau\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    if tau == 1:\n",
    "       Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "    \n",
    "    s = tmdp.reset()\n",
    "    M = 20000\n",
    "    ret = 0\n",
    "    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "    Q = Q_learning(tmdp, s, a, Q, M)\n",
    "\n",
    "\n",
    "print(\"Q function learned with transfer learning:\\n\", Q)\n",
    "\n",
    "pi = get_policy(Q)\n",
    "print(pi)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
