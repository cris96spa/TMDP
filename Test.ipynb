{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimare Q function ottimale\n",
    "## 1. Quanto tempo ci mette per convergere a Q*, al variare di tau\n",
    "## 2. Distanza tra Q* e Q_t \n",
    "## 3. distanza tra Q_0 e Q_0 appresa tramite i valori di tau\n",
    "## 4. cercare di integrare i bound trovati"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrarre policy da Q\n",
    "### Confrontare due diverse Q valutando diverse metriche \n",
    "### Calcolare Q*\n",
    "### Tentativo di implementazione di un curriculum\n",
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from TMDP import TMDP\n",
    "from river_swim import River\n",
    "\n",
    "from algorithms import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with tau=0.9\n",
    "nS = 8\n",
    "gamma = 0.9\n",
    "river = River(nS, gamma, 50, 1000)\n",
    "tau = 0.9\n",
    "xi = np.ones(river.nS)*1/river.nS\n",
    "tmdp = TMDP(river, xi, tau, gamma)\n",
    "tmdp_0 = TMDP(river, xi, 0, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q function with tau: 0.9\n",
      "\n",
      "[[27.65858588  7.86464617]\n",
      " [ 9.7656388   6.15494918]\n",
      " [ 6.62316931 10.60300917]\n",
      " [10.01960772  6.33329262]\n",
      " [10.80757137  6.43258808]\n",
      " [ 6.28236037 10.39402085]\n",
      " [ 9.51543413  6.928138  ]\n",
      " [ 8.98908617  6.32903213]]\n",
      "[0, 0, 1, 0, 0, 1, 0, 0]\n",
      "Q function with tau: 0\n",
      "\n",
      "[[500.         434.34353862]\n",
      " [450.         410.44097761]\n",
      " [405.         371.8386064 ]\n",
      " [364.5        333.53661187]\n",
      " [328.05       324.63096015]\n",
      " [  0.           0.        ]\n",
      " [  0.           0.        ]\n",
      " [  0.           0.        ]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Q function with bellman operator\n",
      "\n",
      "[[500.         436.5       ]\n",
      " [450.         397.35      ]\n",
      " [405.         357.615     ]\n",
      " [364.5        325.61947683]\n",
      " [328.05       341.99806235]\n",
      " [307.79825612 461.16336549]\n",
      " [415.04702894 671.68637968]\n",
      " [604.51774171 990.63345095]]\n",
      "[0, 0, 0, 0, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "s = tmdp.reset()\n",
    "M = 1000\n",
    "Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "ret = 0\n",
    "a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "Q = Q_learning(tmdp, s, a, Q, M)\n",
    "\n",
    "print(\"Q function with tau: {}\\n\".format(tmdp.tau))\n",
    "print(Q)\n",
    "print(get_policy(Q))\n",
    "\n",
    "s = tmdp_0.reset()\n",
    "M_0 = 100000\n",
    "Q = np.zeros((tmdp_0.nS, tmdp_0.nA))\n",
    "ret = 0\n",
    "a = eps_greedy(s, Q, 1., tmdp_0.allowed_actions[s.item()])\n",
    "Q = Q_learning(tmdp_0, s, a, Q, M_0)\n",
    "\n",
    "\n",
    "print(\"Q function with tau: {}\\n\".format(tmdp_0.tau))\n",
    "print(Q)\n",
    "print(get_policy(Q))\n",
    "\n",
    "r_s_a = compute_r_s_a(tmdp_0.nS, tmdp_0.nA, tmdp_0.P_mat, tmdp_0.reward)\n",
    "q_star = bellman_optimal_q(tmdp_0.nS, tmdp_0.nA, tmdp_0.P_mat, tmdp_0.reward, 100000, gamma)\n",
    "\n",
    "\n",
    "print(\"Q function with bellman operator\\n\")\n",
    "print(q_star)\n",
    "print(get_policy(q_star))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xi = np.ones(river.nS)*1/river.nS\\ngamma = 0.9\\nriver = River(gamma)\\n\\nfor tau in taus:\\n    tmdp = TMDP(river, xi, tau, gamma)\\n    s = tmdp.reset()\\n    M = 10000\\n    Q = np.zeros((tmdp.nS, tmdp.nA))\\n    ret = 0\\n    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\\n    Q = Q_learning(tmdp, s, a, Q, M)\\n    Qs.append({\"tau\":tau, \"Q_function\":Q, \"env\":tmdp})\\n\\nfor i in range(len(taus)):\\n    print(\"Tau:\", Qs[i][\\'tau\\'])\\n    print(Qs[i][\\'Q_function\\']) '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" xi = np.ones(river.nS)*1/river.nS\n",
    "gamma = 0.9\n",
    "river = River(gamma)\n",
    "\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    s = tmdp.reset()\n",
    "    M = 10000\n",
    "    Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "    ret = 0\n",
    "    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "    Q = Q_learning(tmdp, s, a, Q, M)\n",
    "    Qs.append({\"tau\":tau, \"Q_function\":Q, \"env\":tmdp})\n",
    "\n",
    "for i in range(len(taus)):\n",
    "    print(\"Tau:\", Qs[i]['tau'])\n",
    "    print(Qs[i]['Q_function']) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" pi = get_policy(Qs[-2]['Q_function'])\\npi_prime = get_policy(Qs[-1]['Q_function'])\\n\\nprint(pi)\\nprint(pi_prime) \""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" pi = get_policy(Qs[-2]['Q_function'])\n",
    "pi_prime = get_policy(Qs[-1]['Q_function'])\n",
    "\n",
    "print(pi)\n",
    "print(pi_prime) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q function learned with transfer learning:\n",
      " [[500.         436.02273663]\n",
      " [450.         388.33373202]\n",
      " [405.         337.72713859]\n",
      " [364.5        322.89000421]\n",
      " [328.05       301.35897062]\n",
      " [295.2449986  268.10554084]\n",
      " [248.69454613 527.79363875]\n",
      " [392.79376009 820.48769454]]\n",
      "[0, 0, 0, 0, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "taus = [1 - i*0.1 for i in range(10)]\n",
    "taus.append(0)\n",
    "\n",
    "# Curriculul for decreasing values of tau\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    if tau == 1:\n",
    "       Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "    \n",
    "    s = tmdp.reset()\n",
    "    M = 500000\n",
    "    ret = 0\n",
    "    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "    Q = Q_learning(tmdp, s, a, Q, M)\n",
    "\n",
    "\n",
    "print(\"Q function learned with transfer learning:\\n\", Q)\n",
    "\n",
    "pi = get_policy(Q)\n",
    "print(pi)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
