{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimare Q function ottimale\n",
    "## 1. Quanto tempo ci mette per convergere a Q*, al variare di tau\n",
    "## 2. Distanza tra Q* e Q_t \n",
    "## 3. distanza tra Q_0 e Q_0 appresa tramite i valori di tau\n",
    "## 4. cercare di integrare i bound trovati"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrarre policy da Q\n",
    "### Confrontare due diverse Q valutando diverse metriche \n",
    "### Calcolare Q*\n",
    "### Tentativo di implementazione di un curriculum\n",
    "### Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from TMDP import TMDP\n",
    "from river_swim import River\n",
    "\n",
    "from algorithms import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with tau=0.9\n",
    "nS = 8\n",
    "gamma = 0.9\n",
    "river = River(nS, gamma, 5, 1000)\n",
    "tau = 0.9\n",
    "xi = np.ones(river.nS)*1/river.nS\n",
    "tmdp = TMDP(river, xi, tau, gamma)\n",
    "tmdp_0 = TMDP(river, xi, 0, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q function with tau: 0\n",
      "\n",
      "[[50.         41.88048962]\n",
      " [45.         38.21911696]\n",
      " [40.5        34.98075486]\n",
      " [36.45       32.65763077]\n",
      " [31.14259381  0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]\n",
      " [ 0.          0.        ]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Q function with tau: 0\n",
      "[[50.         43.66722367]\n",
      " [45.         39.98912165]\n",
      " [40.5        36.4497009 ]\n",
      " [36.45       30.54654188]\n",
      " [32.805      27.55583569]\n",
      " [29.52410382 21.41201226]\n",
      " [26.08870479 21.35597274]\n",
      " [ 0.          0.        ]]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      " Q function with bellman operator\n",
      "[[ 64.33978241  66.4264707 ]\n",
      " [ 59.78382363  92.64863548]\n",
      " [ 83.38377194 137.08716883]\n",
      " [123.37845195 203.98538599]\n",
      " [183.58684739 303.06311755]\n",
      " [272.75680579 449.4704471 ]\n",
      " [404.52340239 665.78575189]\n",
      " [599.2071767  985.43074649]]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "s = tmdp.reset()\n",
    "M = 10000\n",
    "Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "ret = 0\n",
    "a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "Q = Q_learning(tmdp, s, a, Q, M)\n",
    "\n",
    "print(\"\\nQ function with tau: {}\\n\".format(tmdp.tau))\n",
    "print(Q)\n",
    "print(get_policy(Q))\n",
    "\n",
    "s = tmdp_0.reset()\n",
    "M_0 = 100000\n",
    "Q = np.zeros((tmdp_0.nS, tmdp_0.nA))\n",
    "ret = 0\n",
    "a = eps_greedy(s, Q, 1., tmdp_0.allowed_actions[s.item()])\n",
    "Q = Q_learning(tmdp_0, s, a, Q, M_0)\n",
    "\n",
    "\n",
    "print(\"Q function with tau: {}\".format(tmdp_0.tau))\n",
    "print(Q)\n",
    "print(get_policy(Q))\n",
    "\n",
    "r_s_a = compute_r_s_a(tmdp_0.nS, tmdp_0.nA, tmdp_0.P_mat, tmdp_0.reward)\n",
    "q_star = bellman_optimal_q(tmdp_0.nS, tmdp_0.nA, tmdp_0.P_mat, tmdp_0.reward, 1, gamma)\n",
    "\n",
    "\n",
    "print(\"\\n Q function with bellman operator\")\n",
    "print(q_star)\n",
    "print(get_policy(q_star))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xi = np.ones(river.nS)*1/river.nS\\ngamma = 0.9\\nriver = River(gamma)\\n\\nfor tau in taus:\\n    tmdp = TMDP(river, xi, tau, gamma)\\n    s = tmdp.reset()\\n    M = 10000\\n    Q = np.zeros((tmdp.nS, tmdp.nA))\\n    ret = 0\\n    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\\n    Q = Q_learning(tmdp, s, a, Q, M)\\n    Qs.append({\"tau\":tau, \"Q_function\":Q, \"env\":tmdp})\\n\\nfor i in range(len(taus)):\\n    print(\"Tau:\", Qs[i][\\'tau\\'])\\n    print(Qs[i][\\'Q_function\\']) '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\" xi = np.ones(river.nS)*1/river.nS\n",
    "gamma = 0.9\n",
    "river = River(gamma)\n",
    "\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    s = tmdp.reset()\n",
    "    M = 10000\n",
    "    Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "    ret = 0\n",
    "    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "    Q = Q_learning(tmdp, s, a, Q, M)\n",
    "    Qs.append({\"tau\":tau, \"Q_function\":Q, \"env\":tmdp})\n",
    "\n",
    "for i in range(len(taus)):\n",
    "    print(\"Tau:\", Qs[i]['tau'])\n",
    "    print(Qs[i]['Q_function']) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" pi = get_policy(Qs[-2]['Q_function'])\\npi_prime = get_policy(Qs[-1]['Q_function'])\\n\\nprint(pi)\\nprint(pi_prime) \""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" pi = get_policy(Qs[-2]['Q_function'])\n",
    "pi_prime = get_policy(Qs[-1]['Q_function'])\n",
    "\n",
    "print(pi)\n",
    "print(pi_prime) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q function learned with transfer learning:\n",
      " [[  50.           44.02305276]\n",
      " [  45.           41.40971645]\n",
      " [  40.67673115   64.72405776]\n",
      " [  60.39408701  137.53374947]\n",
      " [ 125.70784313  201.07169321]\n",
      " [ 268.87396768  271.0882225 ]\n",
      " [ 325.73465883  418.2877229 ]\n",
      " [ 575.78684033 1855.71395599]]\n"
     ]
    }
   ],
   "source": [
    "taus = [1 - i*0.1 for i in range(10)]\n",
    "taus.append(0)\n",
    "\n",
    "# Curriculul for decreasing values of tau\n",
    "for tau in taus:\n",
    "    tmdp = TMDP(river, xi, tau, gamma)\n",
    "    if tau == 1:\n",
    "       Q = np.zeros((tmdp.nS, tmdp.nA))\n",
    "    \n",
    "    s = tmdp.reset()\n",
    "    M = 10000\n",
    "    ret = 0\n",
    "    a = eps_greedy(s, Q, 1., tmdp.allowed_actions[s.item()])\n",
    "    Q = Q_learning(tmdp, s, a, Q, M)\n",
    "\n",
    "\n",
    "print(\"Q function learned with transfer learning:\\n\", Q)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 1, 1, 1, 1, 1]\n",
      "[1. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "50.000000000000014"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pi = get_policy(Q)\n",
    "print(pi)\n",
    "get_expected_avg_reward(tmdp.nS, tmdp.nA, tmdp.P_mat_tau, pi, tmdp.reward, tmdp.gamma, tmdp.mu)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
